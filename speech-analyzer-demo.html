<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ADHARA ‚Äî Speech Analyzer v2</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', system-ui, sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e1b4b 100%);
            min-height: 100vh;
            color: #f1f5f9;
            padding: 16px;
        }

        .container {
            max-width: 1500px;
            margin: 0 auto;
        }

        header {
            text-align: center;
            margin-bottom: 20px;
        }

        h1 {
            font-size: 1.8rem;
            background: linear-gradient(90deg, #10b981, #06b6d4, #8b5cf6);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            color: #94a3b8;
            font-size: 0.85rem;
            margin-top: 4px;
        }

        .main-grid {
            display: grid;
            grid-template-columns: 1fr 400px;
            gap: 16px;
        }

        .card {
            background: rgba(15, 23, 42, 0.95);
            border: 1px solid rgba(71, 85, 105, 0.5);
            border-radius: 12px;
            padding: 16px;
        }

        .card-title {
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 12px;
            color: #94a3b8;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        /* Waveform */
        .waveform-container {
            position: relative;
            background: #000;
            border-radius: 10px;
            overflow: hidden;
            height: 180px;
        }

        #waveformCanvas {
            width: 100%;
            height: 100%;
        }

        .waveform-overlay {
            position: absolute;
            top: 10px;
            left: 10px;
            right: 10px;
            display: flex;
            justify-content: space-between;
        }

        .status-badge {
            background: rgba(0, 0, 0, 0.75);
            padding: 6px 12px;
            border-radius: 6px;
            font-size: 0.75rem;
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #64748b;
        }

        .status-dot.recording {
            background: #ef4444;
            animation: pulse 1s infinite;
        }

        .status-dot.ready {
            background: #10b981;
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.5;
            }
        }

        /* Controls */
        .controls {
            display: flex;
            gap: 8px;
            margin-top: 12px;
        }

        .btn {
            flex: 1;
            padding: 12px;
            border-radius: 8px;
            font-weight: 600;
            cursor: pointer;
            border: none;
            font-size: 0.9rem;
        }

        .btn-start {
            background: linear-gradient(135deg, #10b981, #059669);
            color: white;
        }

        .btn-stop {
            background: linear-gradient(135deg, #ef4444, #dc2626);
            color: white;
        }

        .btn-reset {
            background: rgba(71, 85, 105, 0.6);
            color: #e2e8f0;
        }

        .btn:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }

        /* Volume Meter */
        .volume-meter {
            height: 8px;
            background: #1e293b;
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }

        .volume-fill {
            height: 100%;
            background: linear-gradient(90deg, #10b981, #06b6d4);
            transition: width 0.05s;
            border-radius: 4px;
        }

        /* Transcript */
        .transcript-box {
            background: rgba(0, 0, 0, 0.4);
            border-radius: 8px;
            padding: 12px;
            min-height: 150px;
            max-height: 250px;
            overflow-y: auto;
            font-size: 0.9rem;
            line-height: 1.7;
            color: #e2e8f0;
        }

        .transcript-box .interim {
            color: #94a3b8;
            font-style: italic;
        }

        .transcript-box .filler {
            background: rgba(245, 158, 11, 0.3);
            padding: 0 4px;
            border-radius: 3px;
            color: #fbbf24;
        }

        .transcript-box .hesitation {
            background: rgba(239, 68, 68, 0.3);
            padding: 0 4px;
            border-radius: 3px;
            color: #f87171;
        }

        .transcript-box .pause-marker {
            color: #8b5cf6;
            font-weight: bold;
        }

        /* Metrics */
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 8px;
            margin-bottom: 12px;
        }

        .metric {
            background: rgba(30, 41, 59, 0.8);
            border-radius: 8px;
            padding: 10px;
            text-align: center;
        }

        .metric-value {
            font-size: 1.3rem;
            font-weight: 700;
            font-family: monospace;
        }

        .metric-label {
            font-size: 0.65rem;
            color: #64748b;
            margin-top: 2px;
        }

        .metric-value.low {
            color: #10b981;
        }

        .metric-value.medium {
            color: #f59e0b;
        }

        .metric-value.high {
            color: #ef4444;
        }

        /* Friction Display */
        .friction-display {
            text-align: center;
            padding: 16px;
            border-radius: 8px;
            margin-bottom: 12px;
        }

        .friction-display.low {
            background: rgba(16, 185, 129, 0.2);
            border: 2px solid #10b981;
        }

        .friction-display.medium {
            background: rgba(245, 158, 11, 0.2);
            border: 2px solid #f59e0b;
        }

        .friction-display.high {
            background: rgba(239, 68, 68, 0.2);
            border: 2px solid #ef4444;
        }

        .friction-level {
            font-size: 1.8rem;
            font-weight: 800;
        }

        .friction-label {
            font-size: 0.8rem;
            color: #94a3b8;
        }

        /* AI Panel */
        .ai-panel {
            border: 2px solid #8b5cf6;
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.08), rgba(236, 72, 153, 0.05));
        }

        .ai-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }

        .ai-status {
            display: flex;
            align-items: center;
            gap: 5px;
            font-size: 0.7rem;
            color: #94a3b8;
        }

        .ai-summary {
            background: rgba(0, 0, 0, 0.35);
            border-radius: 8px;
            padding: 12px;
            min-height: 100px;
            font-size: 0.85rem;
            line-height: 1.6;
            color: #e2e8f0;
        }

        /* Detection Log */
        .detection-log {
            max-height: 140px;
            overflow-y: auto;
        }

        .detection-item {
            display: flex;
            justify-content: space-between;
            padding: 6px 8px;
            background: rgba(30, 41, 59, 0.5);
            border-radius: 4px;
            margin-bottom: 4px;
            font-size: 0.75rem;
        }

        .detection-item.filler {
            border-left: 3px solid #f59e0b;
        }

        .detection-item.hesitation {
            border-left: 3px solid #ef4444;
        }

        .detection-item.pause {
            border-left: 3px solid #8b5cf6;
        }

        .detection-item.lag {
            border-left: 3px solid #06b6d4;
        }

        .age-select {
            background: rgba(30, 41, 59, 0.8);
            border: 1px solid rgba(71, 85, 105, 0.5);
            border-radius: 6px;
            padding: 8px 12px;
            color: #e2e8f0;
            font-size: 0.85rem;
            width: 100%;
            margin-bottom: 12px;
        }

        .disclaimer {
            margin-top: 12px;
            padding: 10px;
            background: rgba(245, 158, 11, 0.1);
            border: 1px solid rgba(245, 158, 11, 0.3);
            border-radius: 6px;
            font-size: 0.75rem;
            color: #fbbf24;
        }

        /* Browser warning */
        .browser-warning {
            background: rgba(239, 68, 68, 0.2);
            border: 1px solid #ef4444;
            border-radius: 8px;
            padding: 12px;
            margin-bottom: 12px;
            font-size: 0.85rem;
            display: none;
        }

        /* Export data button */
        .export-section {
            margin-top: 12px;
            text-align: center;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <h1>üé§ ADHARA Speech Analyzer v2</h1>
            <p class="subtitle">Real-time speech pattern analysis with hesitation & lag detection</p>
        </header>

        <div class="browser-warning" id="browserWarning">
            ‚ö†Ô∏è Speech recognition works best in <strong>Google Chrome</strong>.
            Please open this page in Chrome for full functionality.
        </div>

        <div class="main-grid">
            <!-- Left: Audio Capture -->
            <div>
                <div class="card">
                    <div class="card-title">üéôÔ∏è Audio Capture</div>
                    <div class="waveform-container">
                        <canvas id="waveformCanvas"></canvas>
                        <div class="waveform-overlay">
                            <div class="status-badge">
                                <span class="status-dot" id="statusDot"></span>
                                <span id="statusText">Ready</span>
                            </div>
                            <div class="status-badge" id="durationDisplay">00:00</div>
                        </div>
                    </div>

                    <div style="margin-top: 12px;">
                        <div
                            style="display: flex; justify-content: space-between; font-size: 0.75rem; margin-bottom: 4px;">
                            <span>üîä Volume Level</span>
                            <span id="volumeValue">0%</span>
                        </div>
                        <div class="volume-meter">
                            <div class="volume-fill" id="volumeFill" style="width: 0%"></div>
                        </div>
                    </div>

                    <div class="controls">
                        <button class="btn btn-start" id="startBtn">üé§ START</button>
                        <button class="btn btn-stop" id="stopBtn" disabled>‚èπ STOP</button>
                        <button class="btn btn-reset" id="resetBtn">‚Ü∫ RESET</button>
                    </div>
                </div>

                <div class="card" style="margin-top: 12px;">
                    <div class="card-title">üìù Live Transcript</div>
                    <div class="transcript-box" id="transcript">
                        <span style="color: #64748b;">Click START and begin speaking. Your speech will appear here with
                            highlighted patterns...</span>
                    </div>
                </div>

                <div class="disclaimer">
                    ‚ö†Ô∏è <strong>Demo Only:</strong> Speech recognition uses browser APIs (Chrome recommended).
                    Results vary by accent, mic quality, and environment. Not for assessment purposes.
                </div>
            </div>

            <!-- Right: Analysis -->
            <div>
                <!-- Age Selection -->
                <div class="card">
                    <div class="card-title">üë§ Learner Profile</div>
                    <select class="age-select" id="ageSelect">
                        <option value="6-8">Age 6-8 (Early Elementary)</option>
                        <option value="9-11" selected>Age 9-11 (Upper Elementary)</option>
                        <option value="12-14">Age 12-14 (Middle School)</option>
                        <option value="15+">Age 15+ (High School+)</option>
                    </select>
                </div>

                <!-- Friction Level -->
                <div class="card" style="margin-top: 12px;">
                    <div class="friction-display low" id="frictionDisplay">
                        <div class="friction-level" id="frictionLevel">READY</div>
                        <div class="friction-label">Speech Friction Level</div>
                    </div>

                    <div class="metrics-grid">
                        <div class="metric">
                            <div class="metric-value low" id="speechRate">0</div>
                            <div class="metric-label">WORDS/MIN</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value low" id="fillerCount">0</div>
                            <div class="metric-label">FILLERS</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value low" id="hesitationCount">0</div>
                            <div class="metric-label">HESITATIONS</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value low" id="pauseCount">0</div>
                            <div class="metric-label">LONG PAUSES</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value low" id="lagCount">0</div>
                            <div class="metric-label">LAG/DELAYS</div>
                        </div>
                        <div class="metric">
                            <div class="metric-value low" id="wordCount">0</div>
                            <div class="metric-label">WORDS</div>
                        </div>
                    </div>
                </div>

                <!-- AI Analysis -->
                <div class="card ai-panel" style="margin-top: 12px;">
                    <div class="ai-header">
                        <div class="card-title" style="margin: 0;">ü§ñ AI Analysis</div>
                        <div class="ai-status">
                            <span class="status-dot" id="aiDot"></span>
                            <span id="aiStatus">Connecting...</span>
                        </div>
                    </div>
                    <div class="ai-summary" id="aiSummary">
                        Start speaking to receive AI-powered speech friction analysis...
                    </div>
                </div>

                <!-- Detection Log -->
                <div class="card" style="margin-top: 12px;">
                    <div class="card-title">üìã Detection Log</div>
                    <div class="detection-log" id="detectionLog">
                        <div style="color: #64748b; text-align: center; padding: 16px; font-size: 0.8rem;">
                            Speech patterns will be detected here
                        </div>
                    </div>
                </div>

                <!-- Export for Combined Analysis -->
                <div class="export-section">
                    <button class="btn btn-reset" id="exportBtn"
                        style="flex: none; padding: 8px 16px; font-size: 0.8rem;">
                        üì§ Export Data (for combined analysis)
                    </button>
                </div>
            </div>
        </div>
    </div>

    <script>
        // ============================================
        // ADHARA SPEECH ANALYZER v2
        // Enhanced with hesitation, lag, and combined analysis export
        // ============================================

        const OLLAMA_URL = 'http://localhost:11434';
        const MODEL = 'qwen2.5-coder:7b-instruct-q4_K_M';
        const AI_INTERVAL = 4000; // 4 seconds

        // Filler words and hesitation sounds
        const FILLER_WORDS = ['um', 'uh', 'er', 'ah', 'like', 'you know', 'basically', 'actually', 'literally', 'so', 'well', 'right', 'okay'];
        const HESITATION_PATTERNS = ['uhh', 'umm', 'uhm', 'hmm', 'ahh', 'err', 'mmm', 'erm', 'uuh'];

        // Age-based baselines
        const AGE_BASELINES = {
            '6-8': { avgSpeechRateWPM: 100, avgPauseDurationMs: 800, avgFillerWords: 5, avgHesitations: 3 },
            '9-11': { avgSpeechRateWPM: 120, avgPauseDurationMs: 500, avgFillerWords: 3, avgHesitations: 2 },
            '12-14': { avgSpeechRateWPM: 140, avgPauseDurationMs: 400, avgFillerWords: 2, avgHesitations: 1 },
            '15+': { avgSpeechRateWPM: 150, avgPauseDurationMs: 300, avgFillerWords: 1, avgHesitations: 1 },
        };

        // State
        let isRecording = false;
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let recognition = null;
        let startTime = null;
        let lastAiCall = 0;
        let lastSpeechTime = 0;
        let consecutiveSilenceFrames = 0;

        // Metrics (exportable for combined analysis)
        let metrics = {
            wordCount: 0,
            speechRate: 0,
            fillerCount: 0,
            hesitationCount: 0,
            pauseCount: 0,
            lagCount: 0,
            totalTime: 0,
            silenceTime: 0,
            words: [],
            detections: [],
            timestamps: []
        };

        let finalTranscript = '';
        let interimTranscript = '';

        const canvas = document.getElementById('waveformCanvas');
        const ctx = canvas.getContext('2d');
        let animationId = null;

        // ============================================
        // BROWSER CHECK
        // ============================================

        function checkBrowser() {
            const isChrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor);
            const isEdge = /Edg/.test(navigator.userAgent);

            if (!isChrome && !isEdge) {
                document.getElementById('browserWarning').style.display = 'block';
            }

            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                document.getElementById('transcript').innerHTML =
                    '<span style="color: #ef4444;">‚ùå Speech recognition not supported. Please use Google Chrome.</span>';
                document.getElementById('startBtn').disabled = true;
                return false;
            }
            return true;
        }

        // ============================================
        // INITIALIZATION
        // ============================================

        function init() {
            resizeCanvas();
            window.addEventListener('resize', resizeCanvas);

            if (checkBrowser()) {
                testOllama();
            }

            document.getElementById('startBtn').addEventListener('click', startRecording);
            document.getElementById('stopBtn').addEventListener('click', stopRecording);
            document.getElementById('resetBtn').addEventListener('click', resetAll);
            document.getElementById('exportBtn').addEventListener('click', exportData);
        }

        function resizeCanvas() {
            const container = canvas.parentElement;
            canvas.width = container.clientWidth;
            canvas.height = container.clientHeight;
        }

        async function testOllama() {
            try {
                const res = await fetch(`${OLLAMA_URL}/api/tags`);
                if (res.ok) {
                    const data = await res.json();
                    const hasQwen = data.models?.some(m => m.name.includes('qwen'));
                    document.getElementById('aiDot').className = 'status-dot ready';
                    document.getElementById('aiStatus').textContent = hasQwen ? 'Connected' : 'No Qwen';
                }
            } catch (e) {
                document.getElementById('aiDot').className = 'status-dot';
                document.getElementById('aiStatus').textContent = 'Not connected';
            }
        }

        // ============================================
        // RECORDING
        // ============================================

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                analyser.smoothingTimeConstant = 0.8;
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                isRecording = true;
                startTime = Date.now();
                lastSpeechTime = startTime;
                consecutiveSilenceFrames = 0;

                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                document.getElementById('statusDot').className = 'status-dot recording';
                document.getElementById('statusText').textContent = 'Recording...';
                document.getElementById('transcript').innerHTML = '<span class="interim">Listening...</span>';

                startSpeechRecognition();
                drawWaveform();
                updateDuration();

            } catch (err) {
                console.error('Microphone error:', err);
                document.getElementById('statusText').textContent = 'Mic denied';
                document.getElementById('transcript').innerHTML =
                    '<span style="color: #ef4444;">‚ùå Microphone access denied. Please allow microphone permission.</span>';
            }
        }

        function stopRecording() {
            isRecording = false;

            if (microphone) microphone.disconnect();
            if (audioContext) audioContext.close();
            if (recognition) recognition.stop();
            if (animationId) cancelAnimationFrame(animationId);

            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            document.getElementById('statusDot').className = 'status-dot';
            document.getElementById('statusText').textContent = 'Stopped';

            // Final AI analysis
            if (metrics.wordCount > 3) {
                analyzeWithAI(true);
            }
        }

        function resetAll() {
            stopRecording();
            finalTranscript = '';
            interimTranscript = '';
            metrics = {
                wordCount: 0,
                speechRate: 0,
                fillerCount: 0,
                hesitationCount: 0,
                pauseCount: 0,
                lagCount: 0,
                totalTime: 0,
                silenceTime: 0,
                words: [],
                detections: [],
                timestamps: []
            };

            document.getElementById('transcript').innerHTML = '<span style="color: #64748b;">Click START and begin speaking...</span>';
            document.getElementById('detectionLog').innerHTML = '<div style="color: #64748b; text-align: center; padding: 16px; font-size: 0.8rem;">Speech patterns will be detected here</div>';
            document.getElementById('aiSummary').textContent = 'Start speaking to receive AI analysis...';
            document.getElementById('durationDisplay').textContent = '00:00';
            document.getElementById('frictionLevel').textContent = 'READY';
            document.getElementById('frictionDisplay').className = 'friction-display low';

            updateMetricsDisplay();
            ctx.clearRect(0, 0, canvas.width, canvas.height);
        }

        // ============================================
        // WAVEFORM & VOLUME
        // ============================================

        function drawWaveform() {
            if (!isRecording) return;

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteTimeDomainData(dataArray);

            // Calculate volume
            let sum = 0;
            for (let i = 0; i < bufferLength; i++) {
                const value = (dataArray[i] - 128) / 128;
                sum += value * value;
            }
            const volume = Math.sqrt(sum / bufferLength);
            const volumePercent = Math.min(volume * 400, 100);

            document.getElementById('volumeFill').style.width = volumePercent + '%';
            document.getElementById('volumeValue').textContent = Math.round(volumePercent) + '%';

            // Silence/Lag detection
            const now = Date.now();
            if (volumePercent < 8) {
                consecutiveSilenceFrames++;

                // Detect lag (extended silence during speech)
                if (consecutiveSilenceFrames > 60 && metrics.wordCount > 0) { // ~1 second of silence
                    const silenceDuration = now - lastSpeechTime;
                    if (silenceDuration > 1500 && silenceDuration < 5000) {
                        // Only count as lag if it's been a while since last lag detection
                        const lastLag = metrics.detections.filter(d => d.type === 'lag').pop();
                        if (!lastLag || now - lastLag.time > 3000) {
                            metrics.lagCount++;
                            addDetection('lag', `Speech lag: ${(silenceDuration / 1000).toFixed(1)}s`);
                        }
                    }
                }

                metrics.silenceTime += 16;
            } else {
                if (consecutiveSilenceFrames > 30) { // Was silent for ~0.5s
                    lastSpeechTime = now;
                }
                consecutiveSilenceFrames = 0;
            }

            // Draw waveform
            ctx.fillStyle = 'rgba(0, 0, 0, 0.3)';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            ctx.lineWidth = 2;
            ctx.strokeStyle = volumePercent > 20 ? '#10b981' : '#06b6d4';
            ctx.beginPath();

            const sliceWidth = canvas.width / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = (v * canvas.height) / 2;
                if (i === 0) ctx.moveTo(x, y);
                else ctx.lineTo(x, y);
                x += sliceWidth;
            }

            ctx.lineTo(canvas.width, canvas.height / 2);
            ctx.stroke();

            animationId = requestAnimationFrame(drawWaveform);
        }

        function updateDuration() {
            if (!isRecording) return;

            const elapsed = Math.floor((Date.now() - startTime) / 1000);
            const mins = Math.floor(elapsed / 60).toString().padStart(2, '0');
            const secs = (elapsed % 60).toString().padStart(2, '0');
            document.getElementById('durationDisplay').textContent = `${mins}:${secs}`;

            metrics.totalTime = elapsed;
            if (metrics.totalTime > 0) {
                metrics.speechRate = Math.round((metrics.wordCount / metrics.totalTime) * 60);
            }

            updateMetricsDisplay();

            // AI analysis
            const now = Date.now();
            if (now - lastAiCall > AI_INTERVAL && metrics.wordCount > 3) {
                lastAiCall = now;
                analyzeWithAI(false);
            }

            setTimeout(updateDuration, 1000);
        }

        // ============================================
        // SPEECH RECOGNITION
        // ============================================

        function startSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            recognition.maxAlternatives = 1;

            let lastWord = '';
            let lastWordTime = Date.now();
            let recognitionRestarts = 0;

            recognition.onstart = () => {
                console.log('Speech recognition started');
            };

            recognition.onresult = (event) => {
                interimTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    const confidence = event.results[i][0].confidence;

                    if (event.results[i].isFinal) {
                        const now = Date.now();
                        const pauseDuration = now - lastWordTime;
                        const ageGroup = document.getElementById('ageSelect').value;
                        const baseline = AGE_BASELINES[ageGroup];

                        // Long pause detection
                        if (pauseDuration > baseline.avgPauseDurationMs * 2 && metrics.wordCount > 0) {
                            metrics.pauseCount++;
                            addDetection('pause', `Long pause: ${(pauseDuration / 1000).toFixed(1)}s`);
                        }

                        // Process words
                        const words = transcript.toLowerCase().trim().split(/\s+/);
                        words.forEach(word => {
                            const cleanWord = word.replace(/[^a-z]/g, '');
                            if (cleanWord.length > 0) {
                                metrics.wordCount++;
                                metrics.words.push(cleanWord);
                                metrics.timestamps.push(now);

                                // Filler detection
                                if (FILLER_WORDS.includes(cleanWord)) {
                                    metrics.fillerCount++;
                                    addDetection('filler', `Filler: "${cleanWord}"`);
                                }

                                // Hesitation sounds (uhh, umm, etc.)
                                if (HESITATION_PATTERNS.some(p => cleanWord.includes(p) || cleanWord === p)) {
                                    metrics.hesitationCount++;
                                    addDetection('hesitation', `Hesitation: "${cleanWord}"`);
                                }

                                // Repeated word (stammer)
                                if (cleanWord === lastWord && cleanWord.length > 2) {
                                    metrics.hesitationCount++;
                                    addDetection('hesitation', `Repeated: "${cleanWord}"`);
                                }

                                lastWord = cleanWord;
                            }
                        });

                        finalTranscript += highlightTranscript(transcript) + ' ';
                        lastWordTime = now;
                        lastSpeechTime = now;

                    } else {
                        interimTranscript += transcript;
                    }
                }

                document.getElementById('transcript').innerHTML =
                    finalTranscript +
                    (interimTranscript ? `<span class="interim">${interimTranscript}</span>` : '');

                // Auto-scroll
                const box = document.getElementById('transcript');
                box.scrollTop = box.scrollHeight;
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                if (event.error === 'not-allowed') {
                    document.getElementById('transcript').innerHTML =
                        '<span style="color: #ef4444;">‚ùå Microphone permission denied</span>';
                } else if (event.error === 'no-speech') {
                    // This is normal, just means no speech detected
                    console.log('No speech detected, continuing...');
                }
            };

            recognition.onend = () => {
                if (isRecording && recognitionRestarts < 50) {
                    recognitionRestarts++;
                    setTimeout(() => {
                        if (isRecording) {
                            try {
                                recognition.start();
                            } catch (e) {
                                console.log('Recognition restart failed:', e);
                            }
                        }
                    }, 100);
                }
            };

            try {
                recognition.start();
            } catch (e) {
                console.error('Failed to start recognition:', e);
            }
        }

        function highlightTranscript(text) {
            let result = text;

            // Highlight fillers
            FILLER_WORDS.forEach(filler => {
                const regex = new RegExp(`\\b${filler}\\b`, 'gi');
                result = result.replace(regex, `<span class="filler">${filler}</span>`);
            });

            // Highlight hesitations
            HESITATION_PATTERNS.forEach(pattern => {
                const regex = new RegExp(`\\b${pattern}+\\b`, 'gi');
                result = result.replace(regex, match => `<span class="hesitation">${match}</span>`);
            });

            return result;
        }

        function addDetection(type, message) {
            metrics.detections.push({ type, message, time: Date.now() });

            const log = document.getElementById('detectionLog');
            if (metrics.detections.length === 1) {
                log.innerHTML = '';
            }

            const elapsed = Math.floor((Date.now() - startTime) / 1000);
            const timeStr = `${Math.floor(elapsed / 60)}:${(elapsed % 60).toString().padStart(2, '0')}`;

            log.innerHTML = `<div class="detection-item ${type}">
                <span>${message}</span>
                <span style="color: #64748b;">${timeStr}</span>
            </div>` + log.innerHTML;
        }

        // ============================================
        // METRICS & FRICTION
        // ============================================

        function updateMetricsDisplay() {
            const ageGroup = document.getElementById('ageSelect').value;
            const baseline = AGE_BASELINES[ageGroup];

            document.getElementById('speechRate').textContent = metrics.speechRate;
            document.getElementById('fillerCount').textContent = metrics.fillerCount;
            document.getElementById('hesitationCount').textContent = metrics.hesitationCount;
            document.getElementById('pauseCount').textContent = metrics.pauseCount;
            document.getElementById('lagCount').textContent = metrics.lagCount;
            document.getElementById('wordCount').textContent = metrics.wordCount;

            // Calculate friction
            const friction = calculateFriction(baseline);
            document.getElementById('frictionLevel').textContent = friction.level;
            document.getElementById('frictionDisplay').className = `friction-display ${friction.class}`;

            // Color metrics
            colorMetric('speechRate', Math.abs(metrics.speechRate - baseline.avgSpeechRateWPM) / baseline.avgSpeechRateWPM);
            colorMetric('fillerCount', metrics.fillerCount / Math.max(baseline.avgFillerWords, 1));
            colorMetric('hesitationCount', metrics.hesitationCount / Math.max(baseline.avgHesitations, 1));
        }

        function calculateFriction(baseline) {
            if (metrics.totalTime < 3) return { level: 'READY', class: 'low' };

            const timeMinutes = metrics.totalTime / 60;

            const fillerScore = metrics.fillerCount / Math.max(baseline.avgFillerWords * timeMinutes, 1);
            const hesitationScore = metrics.hesitationCount / Math.max(baseline.avgHesitations * timeMinutes, 1);
            const rateDeviation = Math.abs(metrics.speechRate - baseline.avgSpeechRateWPM) / baseline.avgSpeechRateWPM;
            const lagScore = metrics.lagCount / Math.max(timeMinutes * 2, 1);

            const avgScore = (fillerScore + hesitationScore + rateDeviation + lagScore) / 4;

            if (avgScore > 0.7) return { level: 'HIGH', class: 'high' };
            if (avgScore > 0.3) return { level: 'MEDIUM', class: 'medium' };
            return { level: 'LOW', class: 'low' };
        }

        function colorMetric(id, deviation) {
            const el = document.getElementById(id);
            if (deviation > 0.7) el.className = 'metric-value high';
            else if (deviation > 0.3) el.className = 'metric-value medium';
            else el.className = 'metric-value low';
        }

        // ============================================
        // AI ANALYSIS
        // ============================================

        async function analyzeWithAI(isFinal = false) {
            const aiSummary = document.getElementById('aiSummary');
            const aiDot = document.getElementById('aiDot');
            const aiStatus = document.getElementById('aiStatus');

            if (metrics.wordCount < 3) return;

            aiDot.className = 'status-dot recording';
            aiStatus.textContent = 'Analyzing...';

            const ageGroup = document.getElementById('ageSelect').value;
            const baseline = AGE_BASELINES[ageGroup];
            const friction = calculateFriction(baseline);

            const prompt = `You are ADHARA, an AI helping educators detect speech-based learning friction. Analyze this student's verbal fluency during a learning task.

Age Group: ${ageGroup} years old
Duration: ${metrics.totalTime} seconds

Speech Metrics:
- Words spoken: ${metrics.wordCount} at ${metrics.speechRate} WPM (expected: ${baseline.avgSpeechRateWPM} WPM)
- Filler sounds (um, uh, like): ${metrics.fillerCount}
- Hesitation patterns (uhh, repeated words): ${metrics.hesitationCount}
- Extended pauses: ${metrics.pauseCount}
- Speech delays/lags: ${metrics.lagCount}

Recent detections: ${metrics.detections.slice(-4).map(d => d.message).join('; ') || 'none'}

For the teacher, provide:
1. A 1-sentence assessment of verbal fluency compared to age expectations
2. One specific, actionable suggestion

Do NOT diagnose. Use language like "may indicate" or "appears to suggest".
End with: Friction: ${friction.level}`;

            try {
                const response = await fetch(`${OLLAMA_URL}/api/generate`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        model: MODEL,
                        prompt: prompt,
                        stream: false,
                        options: { temperature: 0.3, num_predict: 120 }
                    }),
                });

                if (!response.ok) throw new Error('API error');

                const data = await response.json();
                aiSummary.textContent = data.response?.trim() || 'No response';
                aiDot.className = 'status-dot ready';
                aiStatus.textContent = 'Connected';

            } catch (error) {
                console.error('AI error:', error);
                aiDot.className = 'status-dot';
                aiStatus.textContent = 'Error';

                // Fallback
                aiSummary.textContent = `Friction Level: ${friction.level}\n\nSpeech rate: ${metrics.speechRate} WPM. Detected ${metrics.fillerCount} fillers, ${metrics.hesitationCount} hesitations, and ${metrics.lagCount} speech lags.`;
            }
        }

        // ============================================
        // EXPORT FOR COMBINED ANALYSIS
        // ============================================

        function exportData() {
            const exportObj = {
                type: 'speech',
                timestamp: new Date().toISOString(),
                duration: metrics.totalTime,
                metrics: {
                    wordCount: metrics.wordCount,
                    speechRate: metrics.speechRate,
                    fillerCount: metrics.fillerCount,
                    hesitationCount: metrics.hesitationCount,
                    pauseCount: metrics.pauseCount,
                    lagCount: metrics.lagCount,
                },
                friction: calculateFriction(AGE_BASELINES[document.getElementById('ageSelect').value]),
                detections: metrics.detections,
                transcript: finalTranscript.replace(/<[^>]*>/g, '') // Strip HTML
            };

            // Store in localStorage for combined analysis
            localStorage.setItem('adhara_speech_data', JSON.stringify(exportObj));

            // Also download as JSON
            const blob = new Blob([JSON.stringify(exportObj, null, 2)], { type: 'application/json' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `adhara-speech-${Date.now()}.json`;
            a.click();

            alert('Speech data exported! Ready for combined analysis.');
        }

        // Initialize
        init();
    </script>
</body>

</html>