# ADHARA
### AI-Powered Learning Friction Detection System

<div align="center">

*Precision AI for early detection of learning difficulties through multimodal behavioral analysis*

[![React](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)](https://reactjs.org/)
[![Vite](https://img.shields.io/badge/Vite-646CFF?style=for-the-badge&logo=vite&logoColor=white)](https://vitejs.dev/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow.js-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/js)
[![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.ai/)

</div>

---

## ğŸ¯ Overview

**ADHARA** (AI-Driven Holistic Assessment for Developmental Recognition and Assistance) is an intelligent early warning system that detects learning friction patterns in children. By analyzing behavioral signals across multiple modalities, ADHARA helps educators identify potential learning difficulties like **Dyslexia**, **Dyscalculia**, and **ADHD** before they become persistent challenges.

> ğŸ’¡ **Why ADHARA?** Traditional assessments only measure results. ADHARA analyzes the *process* of learningâ€”capturing hesitation, stress, attention patterns, and cognitive load in real-time.

### âœ¨ Key Features

- ğŸ§  **Adaptive Learning Sessions** - Questions adjust in real-time based on performance patterns
- ğŸ‘ï¸ **Face & Emotion Analysis** - Detects stress, confusion, and engagement through webcam
- ğŸ–±ï¸ **Mouse Dynamics Tracking** - Analyzes hesitation, jitter, and interaction velocity
- ğŸ¤ **Speech Analysis** - Monitors fluency, stammering, and verbal hesitation patterns
- ğŸ“Š **AI-Powered Reports** - Generates clinical-grade analysis using local LLM (Ollama)
- ğŸ”’ **Privacy-First** - All processing happens locally, no data leaves the machine

---

## ğŸ“ˆ Multimodal Analysis

ADHARA captures four distinct behavioral streams to create a comprehensive friction profile:

| Data Stream | What We Measure | Technology |
|:------------|:----------------|:-----------|
| **Vision** | Gaze tracking, blink rate, attention drift | face-api.js |
| **Emotion** | Micro-expressions, stress detection, engagement | TensorFlow.js |
| **Motor** | Mouse velocity, hesitation patterns, jitter | Custom tracking |
| **Voice** | Fluency, filler words, stammering, pauses | Web Speech API |

---

## ğŸ› ï¸ Installation & Setup Guide

### Prerequisites
Before you begin, ensure you have the following installed:
1. **Node.js** (v18 or higher) - [Download Here](https://nodejs.org/)
2. **Git** - [Download Here](https://git-scm.com/)

### 1. Close the Repository
```bash
git clone https://github.com/JithuMon10/ADHARA-AI-Powered-Learning-Friction.git
cd ADHARA-AI-Powered-Learning-Friction
```

### 2. Install Frontend Dependencies
**Important:** You must move into the `client` directory before installing dependencies.
```bash
cd client
npm install
```

### 3. Start the Application
```bash
npm run dev
```
The application will launch at `http://localhost:5173`.

---

## ğŸ¤– Setting Up AI Analysis (Ollama)

For the detailed "Clinical Analysis Report" to work generates, you need **Ollama** running locally. The app works without it, but the "Analysis" tab will be disabled.

### Recommended Model: Qwen 2.5 7B
We recommend using **Qwen 2.5 7B** for the best balance of performance and reasoning quality for behavior analysis.

### Minimum System Requirements
To run the 7B model locally, your system should meet these specs:
- **RAM:** 8GB minimum (16GB recommended)
- **CPU:** Modern Quad-core processor (Intel i5/Ryzen 5 or newer)
- **GPU (No exceptions):** NVIDIA GPU with 6GB+ VRAM or Apple Silicon (M1/M2/M3) for faster generation
- **Storage:** ~10GB free space

### Setup Steps
1. **Download Ollama** from [ollama.ai](https://ollama.ai).
2. **Install the Model**: Open your terminal/command prompt and run:
   ```bash
   ollama pull qwen2.5:7b
   ```
3. **Run Ollama Server**: You must allow generic origins for the browser to access it:
   ```bash
   # Linux/Mac
   OLLAMA_ORIGINS="*" ollama serve

   # Windows PowerShell
   $env:OLLAMA_ORIGINS="*"; ollama serve
   ```

---

## â“ Troubleshooting

**"Module not found" or "Vite not found"**
> âŒ Error: `'vite' is not recognized` or `missing module face-api.js`
>
> âœ… **Fix:** You likely forgot to `cd client`. Run:
> ```bash
> cd client
> npm install
> npm run dev
> ```

**"AI Analysis is offline" / "Click to generate analysis" does nothing**
> âŒ The button is disabled or says "AI Offline".
>
> âœ… **Fix:** Ensure Ollama is running with `OLLAMA_ORIGINS="*"`. The app needs this to bypass CORS restrictions locally.

**Webcam not working**
> âŒ Browser blocks camera access.
>
> âœ… **Fix:** Check permissions in your browser address bar. Ensure no other app (Zoom/Teams) is using the camera.

---

## ğŸ—ï¸ Architecture

```
ADHARA/
â”œâ”€â”€ client/                    # React + Vite frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/            # Child activity & Teacher dashboard
â”‚   â”‚   â””â”€â”€ utils/            # Analysis modules
â”‚   â”‚       â”œâ”€â”€ faceAnalysis.js      # Emotion & gaze detection
â”‚   â”‚       â”œâ”€â”€ speechAnalysis.js    # Voice pattern analysis
â”‚   â”‚       â””â”€â”€ disorderDetection.js # Clinical pattern matching
â”‚   â””â”€â”€ public/models/        # TensorFlow face detection models
â”œâ”€â”€ data/                     # Baseline datasets
â””â”€â”€ docs/                     # Documentation
```

---

## ğŸ“ Use Cases

| Application | Description |
|-------------|-------------|
| **Schools** | Early screening for learning difficulties |
| **Special Education** | Progress monitoring and intervention planning |
| **Research** | Behavioral data collection for cognitive studies |
| **Parents** | At-home learning pattern awareness |

---

## ğŸ‘¨â€ğŸ’» Team

<div align="center">

| Jithendra V Anand | Aravind Lal |
|:---:|:---:|
| <img src="https://github.com/JithuMon10.png" width="100" style="border-radius:50%"> | <img src="https://github.com/mfscpayload-690.png" width="100" style="border-radius:50%"> |
| [![GitHub](https://img.shields.io/badge/GitHub-JithuMon10-181717?style=flat&logo=github)](https://github.com/JithuMon10) | [![GitHub](https://img.shields.io/badge/GitHub-mfscpayload--690-181717?style=flat&logo=github)](https://github.com/mfscpayload-690) |
| Lead Developer | Contributor |

</div>

---

## ğŸ“„ License

This project is open-source and available under the MIT License.

---

<div align="center">

**Built with â¤ï¸ for early intervention in education**

â­ **Star this repo if you find it useful!** â­

</div>
